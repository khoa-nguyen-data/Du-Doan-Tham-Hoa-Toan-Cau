"""
Dá»° ÄOÃN THIá»†T Háº I Vá»€ NGÆ¯á»œI DO THáº¢M Há»ŒA GÃ‚Y RA
==================================================
Input: Quá»‘c gia, tháº£m há»a, má»©c Ä‘á»™ nghiÃªm trá»ng, thiá»‡t háº¡i kinh táº¿, 
       thá»i gian pháº£n á»©ng, hiá»‡u quáº£ pháº£n á»©ng, kinh Ä‘á»™, vÄ© Ä‘á»™
Output: Sá»‘ ngÆ°á»i bá»‹ cháº¿t do tháº£m há»a
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error
from sklearn.pipeline import Pipeline
import joblib
import warnings

warnings.filterwarnings('ignore')

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")


class DisasterCasualtyPredictor:
    """MÃ´ hÃ¬nh dá»± Ä‘oÃ¡n sá»‘ ngÆ°á»i bá»‹ cháº¿t do tháº£m há»a - Gradient Boosting"""
    
    def __init__(self, csv_path: str):
        self.csv_path = csv_path
        self.df = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.model = None
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.feature_columns = None
        self.feature_importance_df = None
        
    def load_data(self):
        """Load vÃ  khÃ¡m phÃ¡ dá»¯ liá»‡u"""
        print("="*80)
        print("ğŸ“Š BÆ¯á»šC 1: Táº¢I Dá»® LIá»†U")
        print("="*80)
        
        self.df = pd.read_csv(self.csv_path)
        print(f"âœ… Táº£i thÃ nh cÃ´ng {len(self.df):,} báº£n ghi")
        print(f"ğŸ“‹ CÃ¡c cá»™t: {list(self.df.columns)}")
        
        # LÃ m sáº¡ch dá»¯ liá»‡u
        self.df = self.df[self.df['severity_index'] > 0].copy()
        self.df = self.df[self.df['casualties'] >= 0].copy()
        self.df = self.df[self.df['economic_loss_usd'] >= 0].copy()
        
        print(f"âœ… Sau lÃ m sáº¡ch: {len(self.df):,} báº£n ghi")
        
        # Thá»‘ng kÃª
        print(f"\nğŸ“ˆ THá»NG KÃŠ CASUALTIES:")
        print(f"  â€¢ Min: {self.df['casualties'].min():.0f} ngÆ°á»i")
        print(f"  â€¢ Max: {self.df['casualties'].max():.0f} ngÆ°á»i")
        print(f"  â€¢ Mean: {self.df['casualties'].mean():.0f} ngÆ°á»i")
        print(f"  â€¢ Median: {self.df['casualties'].median():.0f} ngÆ°á»i")
        print(f"  â€¢ Std: {self.df['casualties'].std():.0f}")
        
        # Thá»‘ng kÃª theo loáº¡i tháº£m há»a
        print(f"\nğŸ“Š CASUALTIES THEO LOáº I THáº¢M Há»ŒA:")
        disaster_impact = self.df.groupby('disaster_type').agg({
            'casualties': ['count', 'mean', 'sum', 'min', 'max']
        }).round(0)
        print(disaster_impact)
        
        return self.df
    
    def prepare_features(self, df=None, fit_encoders=True):
        """Chuáº©n bá»‹ features vá»›i feature engineering"""
        if df is None:
            df = self.df.copy()
        else:
            df = df.copy()
        
        # Drop non-feature columns
        df = df.drop(columns=['date', 'aid_amount_usd', 'recovery_days', 'continent'], 
                     errors='ignore')
        
        # Target variable
        y = df['casualties'].values
        
        # Feature columns
        feature_cols = ['country', 'disaster_type', 'severity_index', 
                       'economic_loss_usd', 'response_time_hours', 
                       'response_efficiency_score', 'latitude', 'longitude']
        
        X = df[feature_cols].copy()
        
        # Encode categorical variables
        for col in ['country', 'disaster_type']:
            if fit_encoders:
                if col not in self.label_encoders:
                    le = LabelEncoder()
                    X[col] = le.fit_transform(X[col].astype(str))
                    self.label_encoders[col] = le
                else:
                    X[col] = self.label_encoders[col].transform(X[col].astype(str))
            else:
                try:
                    X[col] = self.label_encoders[col].transform(X[col].astype(str))
                except ValueError as e:
                    print(f"âš ï¸  {col} khÃ´ng tÃ¬m tháº¥y trong dá»¯ liá»‡u huáº¥n luyá»‡n")
                    X[col] = -1
        
        self.feature_columns = feature_cols
        return X.values, y
    
    def train(self, test_size=0.2, random_state=42):
        """Huáº¥n luyá»‡n mÃ´ hÃ¬nh"""
        print("\n" + "="*80)
        print("ğŸ”§ BÆ¯á»šC 2: CHUáº¨N Bá»Š VÃ€ HUáº¤N LUYá»†N Dá»® LIá»†U")
        print("="*80)
        
        # Prepare features
        X, y = self.prepare_features(fit_encoders=True)
        
        # Train-test split
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )
        
        print(f"\nğŸ“Š CHIA Dá»® LIá»†U:")
        print(f"  â€¢ Train set: {len(self.X_train):,} máº«u ({(1-test_size)*100:.0f}%)")
        print(f"  â€¢ Test set: {len(self.X_test):,} máº«u ({test_size*100:.0f}%)")
        
        # Scaling
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        # Train Gradient Boosting
        print(f"\nğŸš€ HUáº¤N LUYá»†N MÃ” HÃŒNH GRADIENT BOOSTING...")
        self.model = GradientBoostingRegressor(
            n_estimators=300,
            learning_rate=0.08,
            max_depth=6,
            min_samples_split=5,
            min_samples_leaf=2,
            subsample=0.8,
            random_state=random_state,
            verbose=0
        )
        
        self.model.fit(self.X_train_scaled, self.y_train)
        
        # Evaluate
        return self._evaluate_model()
    
    def _evaluate_model(self):
        """ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh"""
        print("\n" + "="*80)
        print("ğŸ“ˆ BÆ¯á»šC 3: ÄÃNH GIÃ MÃ” HÃŒNH")
        print("="*80)
        
        # Predictions
        y_pred_train = self.model.predict(self.X_train_scaled)
        y_pred_test = self.model.predict(self.X_test_scaled)
        
        # Metrics
        train_r2 = r2_score(self.y_train, y_pred_train)
        test_r2 = r2_score(self.y_test, y_pred_test)
        train_rmse = np.sqrt(mean_squared_error(self.y_train, y_pred_train))
        test_rmse = np.sqrt(mean_squared_error(self.y_test, y_pred_test))
        train_mae = mean_absolute_error(self.y_train, y_pred_train)
        test_mae = mean_absolute_error(self.y_test, y_pred_test)
        train_mape = mean_absolute_percentage_error(self.y_train, y_pred_train)
        test_mape = mean_absolute_percentage_error(self.y_test, y_pred_test)
        
        print(f"\nğŸ“Š Káº¾T QUáº¢ ÄÃNH GIÃ:")
        print(f"\n  {'Metric':<30} {'Train':<20} {'Test':<20}")
        print(f"  {'-'*70}")
        print(f"  {'RÂ² Score':<30} {train_r2:<20.4f} {test_r2:<20.4f}")
        print(f"  {'RMSE (ngÆ°á»i)':<30} {train_rmse:<20.2f} {test_rmse:<20.2f}")
        print(f"  {'MAE (ngÆ°á»i)':<30} {train_mae:<20.2f} {test_mae:<20.2f}")
        print(f"  {'MAPE (%)':<30} {train_mape*100:<20.2f} {test_mape*100:<20.2f}")
        
        # Feature importance
        self._print_feature_importance()
        
        return {
            'train_r2': train_r2, 'test_r2': test_r2,
            'train_rmse': train_rmse, 'test_rmse': test_rmse,
            'train_mae': train_mae, 'test_mae': test_mae,
            'train_mape': train_mape, 'test_mape': test_mape
        }
    
    def _print_feature_importance(self):
        """In táº§m quan trá»ng feature"""
        importance_df = pd.DataFrame({
            'feature': self.feature_columns,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"\nâ­ Táº¦M QUAN TRá»ŒNG CÃC FEATURES:")
        print(f"  {'Feature':<35} {'Importance':<15}")
        print(f"  {'-'*50}")
        for _, row in importance_df.iterrows():
            print(f"  {row['feature']:<35} {row['importance']:.4f}")
        
        self.feature_importance_df = importance_df
    
    def predict(self, country: str, disaster_type: str, severity_index: float,
                economic_loss_usd: float, response_time_hours: float,
                response_efficiency_score: float, latitude: float, 
                longitude: float) -> float:
        """Dá»± Ä‘oÃ¡n cho má»™t báº£n ghi"""
        if self.model is None:
            raise ValueError("âŒ MÃ´ hÃ¬nh chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n!")
        
        input_data = pd.DataFrame({
            'country': [country],
            'disaster_type': [disaster_type],
            'severity_index': [severity_index],
            'economic_loss_usd': [economic_loss_usd],
            'response_time_hours': [response_time_hours],
            'response_efficiency_score': [response_efficiency_score],
            'latitude': [latitude],
            'longitude': [longitude]
        })
        
        # Encode
        for col in ['country', 'disaster_type']:
            try:
                input_data[col] = self.label_encoders[col].transform(
                    input_data[col].astype(str)
                )
            except ValueError:
                print(f"âš ï¸  '{input_data[col].values[0]}' khÃ´ng trong dá»¯ liá»‡u huáº¥n luyá»‡n")
                input_data[col] = -1
        
        # Scale & predict
        input_scaled = self.scaler.transform(input_data)
        prediction = self.model.predict(input_scaled)[0]
        
        return max(0, prediction)
    
    def save_model(self, filepath: str):
        """LÆ°u mÃ´ hÃ¬nh"""
        joblib.dump({
            'model': self.model,
            'scaler': self.scaler,
            'label_encoders': self.label_encoders,
            'feature_columns': self.feature_columns
        }, filepath)
        print(f"âœ… MÃ´ hÃ¬nh Ä‘Ã£ lÆ°u: {filepath}")
    
    def load_model(self, filepath: str):
        """Load mÃ´ hÃ¬nh"""
        data = joblib.load(filepath)
        self.model = data['model']
        self.scaler = data['scaler']
        self.label_encoders = data['label_encoders']
        self.feature_columns = data['feature_columns']
        print(f"âœ… Load mÃ´ hÃ¬nh: {filepath}")


def test_model(csv_path="du_lieu_sach.csv"):
    """Test mÃ´ hÃ¬nh vá»›i cÃ¡c trÆ°á»ng há»£p máº«u"""
    
    predictor = DisasterCasualtyPredictor(csv_path)
    predictor.load_data()
    predictor.train()
    predictor.save_model("disaster_casualty_model.pkl")
    
    print("\n" + "="*80)
    print("ğŸ”® BÆ¯á»šC 4: TEST Dá»° ÄOÃN")
    print("="*80)
    
    test_cases = [
        {
            'country': 'India',
            'disaster_type': 'Earthquake',
            'severity_index': 8.0,
            'economic_loss_usd': 5000000,
            'response_time_hours': 12,
            'response_efficiency_score': 85,
            'latitude': 28.7,
            'longitude': 77.2,
            'description': 'Äá»™ng Ä‘áº¥t máº¡nh táº¡i áº¤n Äá»™'
        },
        {
            'country': 'Philippines',
            'disaster_type': 'Flood',
            'severity_index': 7.5,
            'economic_loss_usd': 3000000,
            'response_time_hours': 8,
            'response_efficiency_score': 90,
            'latitude': 14.5,
            'longitude': 121.0,
            'description': 'LÅ© lá»¥t á»Ÿ Philippines'
        },
        {
            'country': 'Brazil',
            'disaster_type': 'Wildfire',
            'severity_index': 6.5,
            'economic_loss_usd': 2000000,
            'response_time_hours': 16,
            'response_efficiency_score': 75,
            'latitude': -23.55,
            'longitude': -46.6,
            'description': 'ChÃ¡y rá»«ng á»Ÿ Brazil'
        },
        {
            'country': 'Japan',
            'disaster_type': 'Earthquake',
            'severity_index': 9.0,
            'economic_loss_usd': 8000000,
            'response_time_hours': 2,
            'response_efficiency_score': 95,
            'latitude': 35.6762,
            'longitude': 139.6503,
            'description': 'Äá»™ng Ä‘áº¥t cá»±c máº¡nh táº¡i Nháº­t Báº£n'
        }
    ]
    
    for i, case in enumerate(test_cases, 1):
        desc = case.pop('description')
        pred = predictor.predict(**case)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“ TrÆ°á»ng há»£p {i}: {desc}")
        print(f"{'='*80}")
        print(f"  Input:")
        print(f"    ğŸŒ Quá»‘c gia: {case['country']}")
        print(f"    âš ï¸  Tháº£m há»a: {case['disaster_type']}")
        print(f"    ğŸ“Š Má»©c Ä‘á»™: {case['severity_index']}/10")
        print(f"    ğŸ’° Thiá»‡t háº¡i: ${case['economic_loss_usd']:,.0f}")
        print(f"    â±ï¸  Thá»i gian pháº£n á»©ng: {case['response_time_hours']} giá»")
        print(f"    â­ Hiá»‡u quáº£: {case['response_efficiency_score']}/100")
        print(f"    ğŸ“ Vá»‹ trÃ­: ({case['latitude']}, {case['longitude']})")
        print(f"  Output:")
        print(f"    ğŸ‘¥ Dá»° ÄOÃN: {pred:.0f} ngÆ°á»i bá»‹ cháº¿t")


def main():
    """Main menu"""
    print("\n" + "="*80)
    print("ğŸŒ Há»† THá»NG Dá»° ÄOÃN THIá»†T Háº I NGÆ¯á»œI DO THáº¢M Há»ŒA")
    print("="*80)
    print("\n1ï¸âƒ£  Test mÃ´ hÃ¬nh vá»›i cÃ¡c trÆ°á»ng há»£p máº«u")
    print("2ï¸âƒ£  Dá»± Ä‘oÃ¡n tÆ°Æ¡ng tÃ¡c")
    print("3ï¸âƒ£  ThoÃ¡t")
    
    choice = input("\nChá»n (1/2/3): ").strip()
    
    if choice == '1':
        test_model()
    elif choice == '2':
        predictor = DisasterCasualtyPredictor("du_lieu_sach.csv")
        predictor.load_data()
        predictor.train()
        
        countries = sorted(predictor.df['country'].unique())
        disasters = sorted(predictor.df['disaster_type'].unique())
        
        print(f"\nğŸ“ Quá»‘c gia: {', '.join(countries)}")
        print(f"âš ï¸  Tháº£m há»a: {', '.join(disasters)}")
        
        while True:
            print("\n" + "-"*80)
            try:
                c = input("Quá»‘c gia (hoáº·c 'q' thoÃ¡t): ").strip()
                if c.lower() == 'q': break
                d = input("Tháº£m há»a: ").strip()
                s = float(input("Má»©c Ä‘á»™ (0-10): "))
                e = float(input("Thiá»‡t háº¡i (USD): "))
                r = float(input("Thá»i gian pháº£n á»©ng (giá»): "))
                f = float(input("Hiá»‡u quáº£ (0-100): "))
                la = float(input("VÄ© Ä‘á»™: "))
                lo = float(input("Kinh Ä‘á»™: "))
                
                pred = predictor.predict(c, d, s, e, r, f, la, lo)
                print(f"\nâœ… Dá»° ÄOÃN: {pred:.0f} ngÆ°á»i bá»‹ cháº¿t")
            except Exception as ex:
                print(f"âŒ Lá»—i: {ex}")


if __name__ == "__main__":
    main()